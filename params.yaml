
MAX_CONCURRENT_REQUESTS: 100
TEMPERATURE: 0.5

MODEL_NAME_OR_PATH: "meta-llama/Llama-3.1-8B-Instruct"
CACHE_DIR: "downloaded_models/"
LORA_RANK: 16                                               #LORA FLAGS
LORA_ALPHA: 128
LORA_DROPOUT: 0.1  
TRAIN_RATIO: 0.7
VAL_RATIO: 0.15
TEST_RATIO: 0.15
SEED: 42
OUTPUT_DIR: "llama-3.1-8B-finetuned"                        #TRAINING ARGS
EPOCHS: 6
GRADIENT_ACCUMULATION_STEPS: 1 
PER_DEVICE_TRAIN_BATCH_SIZE: 8
PER_DEVICE_EVAL_BATCH_SIZE: 8
GRADIENT_CHECKPOINTING: False
EVAL_STRATEGY: "steps"
EVAL_STEPS: 50
LOGGING_STEPS: 5
SAVE_STRATEGY: "steps"
SAVE_LIMIT: 1
SAVE_STEPS: 100
LR: 1e-4
WARMUP_STEPS: 500
LR_SCHEDULER_TYPE: "cosine"
MAX_GRAD_NORM: 1.0
USE_BEST: True
PUSH_TO_HUB: False
REPORT_TO: "wandb"
DISTRIBUTED_TRAINING: True
CHECKPOINT_PATH: './llama-3.1-8B-finetuned/'
BASELINE: False
USE_QUANTIZATION: False
INFERENCE_SUBSET: ['test']
RESULTS_PATH: 'Inference/'
PER_DEVICE_BATCH_SIZE: 2
MAX_NEW_TOKENS: 210 
TEMPERATURE: 0.5
TOP_P: 0.95
TOP_K: 30
NUM_RETURN_SEQUENCES: 1
NUM_BEAMS: 3
REPETITION_PENALTY: 1.0
NO_REPEAT_NGRAM_SIZE: 0


