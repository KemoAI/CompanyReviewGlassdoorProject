
MAX_CONCURRENT_REQUESTS: 100
TEMPERATURE: 0.5

MODEL_NAME_OR_PATH: "meta-llama/Llama-3.1-8B-Instruct"
CACHE_DIR: "downloaded_models/"
LORA_RANK: 16                                               #LORA FLAGS
LORA_ALPHA: 128
LORA_DROPOUT: 0.1  
TRAIN_RATIO: 0.7
VAL_RATIO: 0.15
TEST_RATIO: 0.15
SEED: 42
OUTPUT_DIR: "llama-3.1-8B-finetuned"                        #TRAINING ARGS
EPOCHS: 6
GRADIENT_ACCUMULATION_STEPS: 1 
PER_DEVICE_TRAIN_BATCH_SIZE: 8
PER_DEVICE_EVAL_BATCH_SIZE: 8
GRADIENT_CHECKPOINTING: False
EVAL_STRATEGY: "steps"
EVAL_STEPS: 50
LOGGING_STEPS: 5
SAVE_STRATEGY: "steps"
SAVE_LIMIT: 1
SAVE_STEPS: 100
LR: 1e-4
WARMUP_STEPS: 500
LR_SCHEDULER_TYPE: "cosine"
MAX_GRAD_NORM: 1.0
USE_BEST: True
PUSH_TO_HUB: False
REPORT_TO: "wandb"
DISTRIBUTED_TRAINING: True

